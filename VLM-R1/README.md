### Set-up
```bash
# set the environment variables as in `.env_example`
source setup.sh
```

### Datasets structure
```bash
ğŸ“¦ data
â”œâ”€â”€ ğŸ“ aokvqa
â”‚   â”œâ”€â”€ ğŸ“ images
â”‚   â”œâ”€â”€ ğŸ“„ train.json
â”‚   â””â”€â”€ ğŸ“„ val.json
â”œâ”€â”€ ğŸ“ DrivingVQA
â”‚   â”œâ”€â”€ ğŸ“„ test.json
â”‚   â””â”€â”€ ğŸ“„ train.json
â”œâ”€â”€ ğŸ“ images
â”œâ”€â”€ ğŸ“ scaleup
â”‚   â”œâ”€â”€ ğŸ“ data
â”‚   â””â”€â”€ ğŸ“ images
â”œâ”€â”€ ğŸ“ scaleup_eval
â”‚   â””â”€â”€ ğŸ“ eval
â”‚       â”œâ”€â”€ ğŸ“ gqa
â”‚       â”œâ”€â”€ ğŸ“ pope
â”‚       â”œâ”€â”€ ğŸ“ scienceqa
â”‚       â”œâ”€â”€ ğŸ“ textvqa
â”‚       â”œâ”€â”€ ğŸ“ vizwiz
â”‚       â””â”€â”€ ğŸ“ vqav2
```

### Train/Eval code structure
```bash
ğŸ“¦ src/open_r1
â”œâ”€â”€ ğŸ“ config
â”œâ”€â”€ ğŸ“ eval     # evaluation code for aokvqa/drivingvqa/viscot
â”œâ”€â”€ ğŸ“ rewards  # rewards definition used in GRPO training
â”œâ”€â”€ ğŸ“ trainer  # GRPO implementation
â”œâ”€â”€ ğŸ“ utils
â”œâ”€â”€ ğŸ“ vlm_modules
â”œâ”€â”€ ğŸ“„ grpo.py  # Training GRPO on aokvqa/drivingvqa datasets
â”œâ”€â”€ ğŸ“„ grpo_scaleup.py  # Training GRPO on VisCOT
â”œâ”€â”€ ğŸ“„ inference.py
â”œâ”€â”€ ğŸ“„ merge_lora.py
â”œâ”€â”€ ğŸ“„ prepare_sft_data.py  # Prepares SFT data format to train with LLlaMA-Factory (drivingvqa, aokvqa)
â””â”€â”€ ğŸ“„ prepare_sft_data_scaleup.py  # Prepares SFT data format to train with LLlaMA-Factory (VisCOT)
```